\section{Enumeration of \glsfmtlongpl{dof}}
\label{sec:enumeration}

Formulating and solving the system of linear equations requires an identification of all involved \glspl{dof} in the global mesh by an integer number.

For \gls{dg} methods, \glspl{dof} on interfaces are attributed to each cell. Thus, the enumeration of both \glspl{dof} and cells happens analogously. However, \gls{cg} methods require that shared \glspl{dof} on interfaces between neighboring cells are uniquely identified. Thus, each \gls{dof} has to be assigned to a single cell, or in other words, will be owned by a single cell.

The latter scenario poses challenges in the enumeration of \glspl{dof} when considering either parallelization or \hp-adaptive methods, let alone a combination of both. A first consideration to cope with this problem would involve so called constraints: We enumerate all \glspl{dof} on each cell independently, but identify two similar \glspl{dof} as identical during the assembly of the equation systems. Although this approach would be easy to implement, we would needlessly add \gls{dof} duplicates to the equation system, sacrificing performance by wasting memory and computing time. We conclude that a unique enumeration of \glspl{dof} is mandatory for a robust \gls{fem} implementation for \gls{cg} methods.

Both \textcite{bangerth2012} and \textcite{bangerth2009} have dealt with \gls{dof} enumeration with parallelization and \hp-adaptive methods, respectively, and presented algorithms for each case, but the combination of both is not trivial. In this section, we will quickly recap each implementation and present an enhanced algorithm in detail for the unique identification of \glspl{dof} for \gls{cg} methods with parallel \hp-adaptive methods.

In the following, all algorithms will be presented independently of the underlying data structures and should be easily applicable on existing code. However, our own implementation in the \dealii{} library relies on all data structures that have been introduced in each of these publications.

%A common approach to parallelization for cell based methods on distributed memory machines is to assign multiple cells to one process of that form a subdomain. Typically during the assembly of the equation system we need to the values of the surrounding cells. Thus, an extension of the subdomain by so called ghosts cells is required.

%When using continuous Galerkin methods on the other hand, \glspl{dof} are shares along interfaces between cells.

%However, a different way of enumerating \glspl{dof} has to be taken with parallel \gls{fem}. \textcite{bangerth2012} provided a

%For \hp-adaptive methods, cells with different finite elements assigned may neighbor each other. Thus, we may also encounter hanging nodes on neighboring cells as depicted in Fig.~\ref{fig:papaptivity} in addition to the ones arising from \h-adaptation.

%In the context of \gls{cg} methods, these hanging nodes from \p-adaptation need to comply to the continuity condition along the faces of a cell.

%For \hp-adaptive methods, cells with different finite elements assigned may neighbor each other. Thus, we may also encounter hanging nodes on neighboring cells as depicted in Fig.~\ref{fig:papaptivity} in addition to the ones arising from \h-adaptation.

%We will also encounter shared dofs on certain neighboring finite elements.

%\textcite{bangerth2009} introduced efficient data structures for \hp-adaptive \gls{fem} codes. We distinguish between objects on the domain to which nodes and \glspl{dof} are assigned, i.e.\@ vertices, lines and quads, and hexes, depending on the dimension of the problem. For each of these object on the domain, a linked list stores the indices of all attached cells and their corresponding \glspl{dof}.

For \hp-adaptive \gls{fem}, the algorithm proposed by \textcite[Sec.~4.2]{bangerth2009} enumerates we will enumerate all \glspl{dof} on each cell consecutively in a first step, and then unifies these shared \glspl{dof} on cell interfaces by keeping the index from the dominating finite element.

%Enumerate degrees of freedom on each cell consecutively. Different indices for degrees of freedom will be assigned on interfaces with neighboring cells. A linked list is introduced for each lowerâ€“dimensional object (i.e. vertices, lines, faces), which stores the indices of all degrees of freedom located on it.

%Unify shared degrees of freedom on interfaces between cells. Depending on the finite elements used on neighboring cells, we identify overlapping degrees of freedom with the previously introduced lists, and merge them by keeping the lower index and eliminating the other.

%Later, they indices of dof duplicates will be unified using finite element, if both finite elements are compatible?

%Since workload is assigned to cells, we will assign \glspl{dof} to cells with a lower number of dofs. To distinguish between cells, we employ finite element domination algorithm.

In parallel applications, the enumeration of \glspl{dof} on interfaces between neighboring subdomains pose a problem. They have to be assigned to one particular subdomain, for which \textcite[Sec.~3.1]{bangerth2012} proposed to use a certain \textit{tie-break} criterion to decide. Their algorithm starts by enumerating all \glspl{dof} on all locally owned cells. On interfaces between subdomains, all \glspl{dof} will be assigned to the process with the lower rank and thus keep the index from the subdomain with the lower identifier. Once ownership of all \glspl{dof} is clarified, all \glspl{dof} on the global domain will be re-enumerated so that every locally owned \gls{dof} has its final index assigned. Each process needs to know all locally relevant \glspl{dof} for the solution of the equation system, which requires exchanging \gls{dof} indices on ghost cells via point-to-point communication. We have to perform this operation twice since there may have been \glspl{dof} on ghost cells that the owning process did not know the correct indices of yet.

For parallel \hp-adaptive \gls{fem}, the mere juxtaposition of both algorithms does not result in a unique enumeration of \glspl{dof}. The case in which distinct finite elements from different subdomains are adjacent has to be given special consideration. We could again cope with this situation by constraining these particular \glspl{dof} to each other. However, this would again leave unnecessary \gls{dof} duplicates in the equation system. Second, the global number of \glspl{dof} would differ with the number of subdomains. We would rather keep it independent from the number of processes, which would simplify debugging and assures that our solvers yield the same results on any amount of subdomains. We developed an algorithm that combined the ideas of both previous algorithms.

To follow the same nomenclature as \cite{bangerth2012}, we call the set of all locally owned cells $\mathbb{T}^p_{loc}$, the set of all ghost cells $\cellsp{ghost}$, and of all locally relevant cells $\cellsp{rel} = \cellsp{loc} \cup \cellsp{ghost}$ on processor and subdomain $p$.

We need to enumerate all \glspl{dof} on locally relevant cells, which includes ghost cells. Thus, we begin by exchanging active fe indices on ghost cells so that we know about all locally relevant finite elements and can prepare all data structures accordingly. We do this with point-to-point communication.

In short, we take most parts of the parallel algorithm. We will add a \gls{dof} unification step after enumerating all locally owned cells, while subjecting to the finite element domination hierarchy to decide ownership on all interfaces. After exchanging \glspl{dof} on all ghost cells, we are left to merge the valid \glspl{dof} on interfaces with the valid counterparts.

In detail, the complete algorithm consists of the following steps, starting with an initialization step flagged with `0`.
\begin{enumerate}
  \item[0.] \textit{Initialization.}
  On all locally relevant cells $K \in \cellsp{rel}$, \gls{dof} indices are set to an invalid value, for example $-1$.
  \item \textit{Local enumeration.}
  Iterate over all locally owned cells $K \in \cellsp{loc}$ and assign valid \gls{dof} indices in ascending order, starting from zero. Remember the number of all assigned \gls{dof} indices as $n_p$.
  \item \textit{Tie-break.}
  Go over all ghost cells $K \in \cellsp{ghost}$. Invalidate all \gls{dof} indices on $K$, if $K$ belongs to a subdomain of lower rank $q < p$, invalidate all \gls{dof} indices on $K$.
  \item \textit{Unification.}
  Unify \glspl{dof}. Also incorporate ghost cells. This has to happen after the tie-break.
\end{enumerate}
At this stage, each processor knows which indices are owned by with him.
\begin{enumerate}[resume]
  \item \textit{Global re-enumeration.}
  Iterate over all locally owned cells $K \in \cellsp{loc}$
  and enumerate those \gls{dof} indices in ascending order that have a valid value assigned. This time, we will not start at zero, but at the number of \glspl{dof} that are owned by all processors of lower rank $q < p$, or in other words, at $\sum_{q=0}^{p-1} n_q$, where $n_q$ is the number of \gls{dof} indices associated with subdomain $q$. This corresponds to a prefix sum or exclusive scan, and can be obtained via \texttt{MPI\_Exscan} \parencite{mpi31}.
\end{enumerate}
At this stage, all subdomains and processors have the correct index assigned to all locally owned \glspl{dof}.
\begin{enumerate}[resume]
  \item \textit{Ghost exchange.}
  Text.
  This involves point-to-point communication using the following steps.
  \begin{enumerate}[label=\alph*.]
    \item Mark all local cells that have ghost neighbors.
    \item Loop over all of these cells and prepare a data container with the cellid and list of corresponding dof indices.
    \item Send this list to all
    with non-blocking point-to-point communication, which can be executed via \texttt{MPI\_ISend} \parencite{mpi31}.
    \item Receive a list of all via non-blocking point-to-point communication. In MPI, which can be executed via \texttt{MPI\_Irecv} \parencite{mpi31}.
  \end{enumerate}
  \item \textit{Merge.}
  After the previous ghost exchange, each support point on interface has exactly one valid \gls{dof} index assigned. Iterate over all interfaces on ghost cells $K \in \cellsp{ghost}$ and set all remaining invalid \gls{dof} indices with the corresponding one of the dominating finite element.
\end{enumerate}
At this stage, all locally owned cells $K \in \cellsp{loc}$ have their correct \gls{dof} indices set.
\begin{enumerate}[resume]
  \item \textit{Ghost exchange.}
  Text.
\end{enumerate}
Now, all global \gls{dof} indices have been set uniquely.


To identify 

\glspl{dof} . Each

Depending on which Galerkin method has been picked, we need to ... .

Continous Galerkin methods require a unique identification of \glspl{dof} on interfaces between cells, because of the continuity requirement on the solution.




The enumeration of dofs faces fundamental problems in both parallelisation and \hp-adaptive methods individually. Let us first elaborate on those and quickly recall the corresponding papers, before moving to the actual consolidation of the two.

Even for \hp-adaptive methods, there are two kinds of issues:

For \h-adaptive methods, neighboring cells. SO called hanging nodes.

For \p-adaptive methods, finite elements of neighboring 

In parallelisation, the workload of ... will be distributed the ownership




Text.


For hp methods, data structures need to be 

For parallel methods, these structures need to be sondistent trhouthgout

The difficulties with both of these methods are

We will first recall

For parallel


For hp adaptive methods, using continous finite elements are crucial

In the following, we present a novel approach to merge both algorithms into one.



This step is essentially similar to...
%%%


Problems that occur involve shared \glspl{dof} on neighboring cells with the same finite element assigned on either the same or different subdomains, as well as with different finite elements assigned on either the same or different subdomains.

A suitable benchmark that we used to test the enumeration algorithm consists of a two-dimensional domain of four neighboring cells. We provide two different Lagrangian elements that share at least one additional \gls{dof} per cell interface than only on vertices. Each of these finite elements will be assigned to two catty-cornered cells. Further, we divide the mesh into two subdomains containing two neighboring cells of differing finite elements. The whole setup is shown in fig.~\ref{fig:enumbenchmark} and covers the aforementioned special cases that we have encountered so far in parallel \hp-adaptive meshes. A step-by-step demonstration of the algorithm on this particular benchmark is presented in app.~\ref{app::enumeration}.

\todo{Add benchmark test figure.}
\begin{figure}
  \centering
  \begin{tikzpicture}[scale=3]
    \def\Length{1}
    \def\Radius{0.03}
    
    \LagrangeCell{0}{0}{\Length}{\Radius}{2}
      {{0,1,2,3,4,5,6,7,8}};
    \LagrangeCell{\Length}{0}{\Length}{\Radius}{4}
      {{1,9,3,51,10,5,11,12,13,14,15,16,17,18,54,19,20,21,22,23,24,25,26,27,28}};
    \LagrangeCell{0}{\Length}{\Length}{\Radius}{4}
      {{2,3,29,30,31,32,33,34,35,36,37,7,38,39,40,41,42,43,44,45,46,47,48,49,50}};
    \LagrangeCell{\Length}{\Length}{\Length}{\Radius}{2}
      {{3,51,30,52,35,53,54,55,56}};
  \end{tikzpicture}
  \caption{Benchmark for the algorithm to enumerate \glspl{dof}.}
  \label{fig:enumbenchmark}
\end{figure}

We do not need more communication steps as the two original ones from the \h-adaptive variant.

We still haven't figured out a solution for the 3D problem with a line facing three or more different finite elements, as explained in \textcite[Sec.~?.?]{bangerth2009}.

There might be chains of constraints that span over \glspl{dof} from multiple processes. To deal with this case, we might need more than the original two communication steps. However, we couldn't think of any scenario in which this is going to be the case, and didn't encounter one in our investigations so far.
