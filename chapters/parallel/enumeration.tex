\section{Enumeration of \glsfmtlongpl{dof}}
\label{sec:enumeration}

Formulating and solving the system of linear equations requires an identification of all involved \glspl{dof} in the global mesh by an integer number.

For \gls{dg} methods, \glspl{dof} on interfaces are attributed to each cell. Thus, the enumeration of both \glspl{dof} and cells happens analogously. However, \gls{cg} methods require that shared \glspl{dof} on interfaces between neighboring cells are uniquely identified. Thus, each \gls{dof} has to be assigned to a single cell, or in other words, will be owned by a single cell.

%Depending on which Galerkin method has been picked, we need to ... .
%Continous Galerkin methods require a unique identification of \glspl{dof} on interfaces between cells, because of the continuity requirement on the solution.

The latter scenario poses challenges in the enumeration of \glspl{dof} when considering either parallelization or \hp-adaptive methods, let alone a combination of both. A first consideration to cope with this problem would involve so called constraints: We enumerate all \glspl{dof} on each cell independently, but identify two similar \glspl{dof} as identical during the assembly of the equation systems. Although this approach would be easy to implement, we would needlessly add \gls{dof} duplicates to the equation system, sacrificing performance by wasting memory and computing time. We conclude that a unique enumeration of \glspl{dof} is mandatory for a robust \gls{fem} implementation for \gls{cg} methods.

Both \textcite{bangerth2012} and \textcite{bangerth2009} have dealt with \gls{dof} enumeration with parallelization and \hp-adaptive methods, respectively, and presented algorithms for each case, but the combination of both is not trivial. In this section, we will quickly recap each implementation and present an enhanced algorithm in detail for the unique identification of \glspl{dof} for \gls{cg} methods with parallel \hp-adaptive methods.

In the following, all algorithms will be presented independently of the underlying data structures and should be easily applicable on existing code. However, our own implementation in the \dealii{} library relies on all data structures that have been introduced in each of these publications.

%A common approach to parallelization for cell based methods on distributed memory machines is to assign multiple cells to one process of that form a subdomain. Typically during the assembly of the equation system we need to the values of the surrounding cells. Thus, an extension of the subdomain by so called ghosts cells is required.

%When using continuous Galerkin methods on the other hand, \glspl{dof} are shares along interfaces between cells.

%However, a different way of enumerating \glspl{dof} has to be taken with parallel \gls{fem}. \textcite{bangerth2012} provided a

%For \hp-adaptive methods, cells with different finite elements assigned may neighbor each other. Thus, we may also encounter hanging nodes on neighboring cells as depicted in Fig.~\ref{fig:papaptivity} in addition to the ones arising from \h-adaptation.

%In the context of \gls{cg} methods, these hanging nodes from \p-adaptation need to comply to the continuity condition along the faces of a cell.

%For \hp-adaptive methods, cells with different finite elements assigned may neighbor each other. Thus, we may also encounter hanging nodes on neighboring cells as depicted in Fig.~\ref{fig:papaptivity} in addition to the ones arising from \h-adaptation.

%We will also encounter shared dofs on certain neighboring finite elements.

%\textcite{bangerth2009} introduced efficient data structures for \hp-adaptive \gls{fem} codes. We distinguish between objects on the domain to which nodes and \glspl{dof} are assigned, i.e.\@ vertices, lines and quads, and hexes, depending on the dimension of the problem. For each of these object on the domain, a linked list stores the indices of all attached cells and their corresponding \glspl{dof}.

%Problems that occur involve shared \glspl{dof} on neighboring cells with the same finite element assigned on either the same or different subdomains, as well as with different finite elements assigned on either the same or different subdomains.

For \hp-adaptive \gls{fem}, the algorithm proposed by \textcite[Sec.~4.2]{bangerth2009} enumerates we will enumerate all \glspl{dof} on each cell consecutively in a first step, and then unifies these shared \glspl{dof} on cell interfaces by keeping the index from the dominating finite element.

%Enumerate degrees of freedom on each cell consecutively. Different indices for degrees of freedom will be assigned on interfaces with neighboring cells. A linked list is introduced for each lowerâ€“dimensional object (i.e. vertices, lines, faces), which stores the indices of all degrees of freedom located on it.

%Unify shared degrees of freedom on interfaces between cells. Depending on the finite elements used on neighboring cells, we identify overlapping degrees of freedom with the previously introduced lists, and merge them by keeping the lower index and eliminating the other.

%Later, they indices of dof duplicates will be unified using finite element, if both finite elements are compatible?

%Since workload is assigned to cells, we will assign \glspl{dof} to cells with a lower number of dofs. To distinguish between cells, we employ finite element domination algorithm.

In parallel applications, the enumeration of \glspl{dof} on interfaces between neighboring subdomains pose a problem. They have to be assigned to one particular subdomain, for which \textcite[Sec.~3.1]{bangerth2012} proposed to use a certain \textit{tie-break} criterion to decide. Their algorithm starts by enumerating all \glspl{dof} on all locally owned cells. On interfaces between subdomains, all \glspl{dof} will be assigned to the process with the lower rank and thus keep the index from the subdomain with the lower identifier. Once ownership of all \glspl{dof} is clarified, all \glspl{dof} on the global domain will be re-enumerated so that every locally owned \gls{dof} has its final index assigned. Each process needs to know all locally relevant \glspl{dof} for the solution of the equation system, which requires exchanging \gls{dof} indices on ghost cells via point-to-point communication. We have to perform this operation twice since there may have been \glspl{dof} on ghost cells that the owning process did not know the correct indices of yet.

%The enumeration of dofs faces fundamental problems in both parallelisation and \hp-adaptive methods individually. Let us first elaborate on those and quickly recall the corresponding papers, before moving to the actual consolidation of the two.

For parallel \hp-adaptive \gls{fem}, the mere juxtaposition of both algorithms does not result in a unique enumeration of \glspl{dof}. The case in which distinct finite elements from different subdomains are adjacent has to be given special consideration. We could again cope with this situation by constraining these particular \glspl{dof} to each other. However, this would again leave unnecessary \gls{dof} duplicates in the equation system. Second, the global number of \glspl{dof} would differ with the number of subdomains. We would rather keep it independent from the number of processes, which would simplify debugging and assures that our solvers yield the same results on any amount of subdomains. We developed an algorithm that combined the ideas of both previous algorithms.

To follow the same nomenclature as \cite{bangerth2012}, we call the set of all locally owned cells $\mathbb{T}^p_{loc}$, the set of all ghost cells $\cellsp{ghost}$, and the set of all locally relevant cells $\cellsp{rel} = \cellsp{loc} \cup \cellsp{ghost}$ on process and subdomain $p$.

We need to enumerate all \glspl{dof} on locally relevant cells, which includes ghost cells. Thus, we begin by exchanging active fe indices on ghost cells so that we know about all locally relevant finite elements and can prepare all data structures accordingly. We do this with point-to-point communication.

In short, we take most parts of the parallel algorithm. We will add a \gls{dof} unification step after enumerating all locally owned cells, while subjecting to the finite element domination hierarchy to decide ownership on all interfaces. After exchanging \glspl{dof} on all ghost cells, we are left to merge the valid \glspl{dof} on interfaces with the valid counterparts.

In detail, the complete algorithm consists of the following six steps, starting with an initialization step flagged with `0`.
\begin{enumerate}
  \item[0.] \textit{Initialization.}
  On all locally relevant cells $K \in \cellsp{rel}$, \gls{dof} indices are set to an invalid value, for example $-1$.
  \item \textit{Local enumeration.}
  Iterate over all locally owned cells $K \in \cellsp{loc}$ and assign valid \gls{dof} indices in ascending order, starting from zero.
  \item \textit{Tie-break.}
  Go over all locally owned cells $K \in \cellsp{loc}$. If $K$ neighbors a ghost cell which has the same finite element assigned and belongs to a subdomain of lower rank $q < p$, then invalidate all \glspl{dof} on their shared interface.
  \item \textit{Unification.}
  Go over all locally relevant cells $K \in \cellsp{rel}$. For all shared \glspl{dof} on interfaces to neighboring cells, assign the index from the dominating finite element to the \gls{dof}. If no dominating index has been found, pick the lower one. On interfaces to ghost cells, unify the indices only if the dominating element is assigned to the ghost cell. We will never overwrite a valid index on ghost interfaces.
%  This has to happen after the tie-break.
\end{enumerate}
At this stage, each process knows which indices are owned by him.
\begin{enumerate}[resume]
  \item \textit{Global re-enumeration.}
  Iterate over all locally owned cells $K \in \cellsp{loc}$
  and enumerate those \gls{dof} indices in ascending order that have a valid value assigned. Remember the number of all valid \gls{dof} indices on this subdomain as $n_p$. In a next step, shift all indices by the number of \glspl{dof} that are owned by all processors of lower rank $q < p$, or in other words, by $\sum_{q=0}^{p-1} n_q$. This corresponds to a prefix sum or exclusive scan, and can be obtained via \texttt{MPI\_Exscan} \textcite{mpi31}.
\end{enumerate}
At this stage, all subdomains and processes have the correct index assigned to all locally owned \glspl{dof}.
\begin{enumerate}[resume]
  \item \textit{Ghost exchange.}
  Communicate \gls{dof} indices from locally owned cells $K \in \cellsp{loc}$ to other processes using point-to-point communication as follows.
  \begin{enumerate}[label=\alph*.]
%    \item Mark all vertices on locally owned cells $\cellsp{loc}$.
%    \item Go over all vertices on ghost cells $\cellsp{ghost}$ and store all marked vertices and their all processor identifiers.
%    \item Mark all locally owned cells that have ghost neighbors.
    \item Prepare containers with data to be sent from this subdomain $p$ to each adjacent subdomain $q$.
    \item Loop over all locally owned cells that have ghost neighbors. Add the cell identifier with all associated \gls{dof} indices to every data container that corresponds to an adjacent subdomain of the current cell.
    \item Send each data container to its designated destination process with non-blocking point-to-point communication via \texttt{MPI\_ISend} \textcite{mpi31}.
    \item Receive data containers from processes of adjacent subdomains via non-blocking point-to-point communication via \texttt{MPI\_Irecv} \textcite{mpi31}. The received data corresponds to ghost cells of this subdomain $p$. On each of these cells, set the received \gls{dof} indices accordingly.
  \end{enumerate}
  All communication in this step is symmetric, which means that a process only receives data from another when it also sends data to it. Thus there is no need to negotiate communication.
  \item \textit{Merge on interfaces.}
  After the previous ghost exchange, each node on interfaces with ghost cells has exactly one valid \gls{dof} index assigned. Go over all locally relevant cells $K \in \cellsp{rel}$. On interfaces between locally owned and ghost cells, set all remaining invalid \gls{dof} indices with the corresponding valid one of the dominating finite element.
\end{enumerate}
At this stage, all locally owned cells $K \in \cellsp{loc}$ have their correct \gls{dof} indices set.
\begin{enumerate}[resume]
  \item \textit{Ghost exchange.}
  Some ghost cells may still have invalid \gls{dof} indices assigned since the owning process did not know all correct indices on this particular cell when the last communication happened. Another ghost exchange closes this gap by repeating step 5. However this time, only data from those cells has to be communicated which had invalid \gls{dof} indices prior to step 5d.
\end{enumerate}
At the end of this algorithm, all global \gls{dof} indices have been set correctly, and every process knows the indices of all locally relevant \glspl{dof}. No new communication steps in addition to the two original ones from the \h-adaptive variant are introduced.

A suitable benchmark that we used to test the enumeration algorithm consists of a two-dimensional domain of four neighboring cells. We provide two different Lagrangian elements that share at least one additional \gls{dof} per cell interface than only on vertices. For this purpose, we choose Lagrangian elements $Q_2$ and $Q_4$. Each of these finite elements will be assigned to two catty-cornered cells. Further, we divide the mesh into two subdomains containing two neighboring cells of differing finite elements. The whole setup is shown in Fig.~\ref{fig:enumbenchmark} and covers all aforementioned hanging node scenarios that we have encountered so far in parallel \hp-adaptive meshes. A step-by-step demonstration of the algorithm on this particular benchmark is presented in app.~\ref{app::enumeration}.

\begin{figure}
\centering
\input{figures/parallel/enumbenchmark}
\input{figures/parallel/enumbenchmarklegend}
\caption{Benchmark for the algorithm to enumerate \glspl{dof}.}
\label{fig:enumbenchmark}
\end{figure}

There might be chains of constraints that span over \glspl{dof} from multiple subdomains. To deal with this case, we might need more than the current two communication steps in our algorithm. However, we couldn't think of any scenario in which this is going to be the case, and didn't encounter one in our investigations so far.

In three-dimensional scenarios, \textcite[Sec.~4.6]{bangerth2009} pointed out possible complications with circular constraints during \gls{dof} unification whenever three different finite elements share a common line. We still haven't figured out a satisfactory solution for this problem and subject to their original way to cope with this situation: All \glspl{dof} on these lines will be excluded from the unification step and will be treated separately.