\section{Enumeration of \glsfmtlongpl{dof}}
\label{sec:enumeration}

Formulating and solving the system of linear equations requires an unique identification of all involved \glspl{dof} in the global mesh.

\Glspl{dof} are associated with mesh objects, i.e., vertices, edges, faces, and cells. If support points are located on interfaces between neighboring cells in the context of \gls{dg} methods, they are assigned separate \glspl{dof} on each cell. Thus, the enumeration of both \glspl{dof} and cells can happen analogously. However, \gls{cg} methods require that shared \glspl{dof} on interfaces between neighboring cells are unique. Thus, each \gls{dof} has to relate to a single cell, or in other words, will be owned by a single cell. This assignment is crucial for the efficient preparation of relevant distributed data structures.

%Depending on which Galerkin method has been picked, we need to ... .
%Continous Galerkin methods require a unique identification of \glspl{dof} on interfaces between cells, because of the continuity requirement on the solution.

The latter scenario poses challenges in the enumeration of \glspl{dof} when considering either parallelization or \hp-adaptive methods, let alone a combination of both. A first attempt to cope with this problem would involve so called constraints: We enumerate all \glspl{dof} on each cell independently, but identify two similar \glspl{dof} as identical during the assembly of the equation system. Although this approach would be easy to implement, we would needlessly add \gls{dof} duplicates to the equation system, sacrificing performance by wasting memory and computing time. We conclude that a unique enumeration of \glspl{dof} is mandatory for a robust \gls{fem} implementation for \gls{cg} methods.

Both \textcite{bangerth2012} and \textcite{bangerth2009} have dealt with \gls{dof} enumeration with parallelization and \hp-adaptive methods, respectively, and presented algorithms for each case, but the combination of both is not trivial. In this section, we will briefly summarize each implementation and present an enhanced algorithm in detail for the unique identification of \glspl{dof} for \gls{cg} methods with parallel \hp-adaptive methods.

In the following, all algorithms will be presented independently of the underlying data structures and should be easily applicable to any kind of existing \gls{fem} code. Results will be presented with our reference implementation in the \dealii{} library.

%A common approach to parallelization for cell based methods on distributed memory machines is to assign multiple cells to one process of that form a subdomain. Typically during the assembly of the equation system we need to the values of the surrounding cells. Thus, an extension of the subdomain by so called ghosts cells is required.

%When using continuous Galerkin methods on the other hand, \glspl{dof} are shares along interfaces between cells.

%However, a different way of enumerating \glspl{dof} has to be taken with parallel \gls{fem}. \textcite{bangerth2012} provided a

%For \hp-adaptive methods, cells with different finite elements assigned may neighbor each other. Thus, we may also encounter hanging nodes on neighboring cells as depicted in Fig.~\ref{fig:papaptivity} in addition to the ones arising from \h-adaptation.

%In the context of \gls{cg} methods, these hanging nodes from \p-adaptation need to comply to the continuity condition along the faces of a cell.

%For \hp-adaptive methods, cells with different finite elements assigned may neighbor each other. Thus, we may also encounter hanging nodes on neighboring cells as depicted in Fig.~\ref{fig:papaptivity} in addition to the ones arising from \h-adaptation.

%We will also encounter shared dofs on certain neighboring finite elements.

%\textcite{bangerth2009} introduced efficient data structures for \hp-adaptive \gls{fem} codes. We distinguish between objects on the domain to which nodes and \glspl{dof} are assigned, i.e.\@ vertices, lines and quads, and hexes, depending on the dimension of the problem. For each of these object on the domain, a linked list stores the indices of all attached cells and their corresponding \glspl{dof}.

%Problems that occur involve shared \glspl{dof} on neighboring cells with the same finite element assigned on either the same or different subdomains, as well as with different finite elements assigned on either the same or different subdomains.

For \hp-adaptive \gls{fem}, the algorithm proposed by \textcite[Sec.~4.2]{bangerth2009} enumerates all \glspl{dof} on each cell consecutively in a first step, and then unifies these shared \glspl{dof} on cell interfaces by favoring the index of the dominating finite element.

%Enumerate degrees of freedom on each cell consecutively. Different indices for degrees of freedom will be assigned on interfaces with neighboring cells. A linked list is introduced for each lowerâ€“dimensional object (i.e. vertices, lines, faces), which stores the indices of all degrees of freedom located on it.

%Unify shared degrees of freedom on interfaces between cells. Depending on the finite elements used on neighboring cells, we identify overlapping degrees of freedom with the previously introduced lists, and merge them by keeping the lower index and eliminating the other.

%Later, they indices of dof duplicates will be unified using finite element, if both finite elements are compatible?

%Since workload is assigned to cells, we will assign \glspl{dof} to cells with a lower number of dofs. To distinguish between cells, we employ finite element domination algorithm.

In parallel applications, the enumeration of \glspl{dof} on interfaces between neighboring subdomains pose a problem: They have to be assigned to one of them, for which \textcite[Sec.~3.1]{bangerth2012} proposed to use a certain \textit{tie-break} criterion as a decision aid. Their algorithm starts with enumerating \glspl{dof} on all locally owned cells. On interfaces between subdomains, all \glspl{dof} will be assigned to the process with the lower rank and thus keep the index from the subdomain with the lower identifier. Once ownership of all \glspl{dof} is clarified, their index will be increased by the number of \glspl{dof} owned by processes with a lower rank. Now, every locally owned \gls{dof} has its final index assigned. Each process needs to know all locally relevant \glspl{dof} for the solution of the equation system, which requires exchanging \gls{dof} indices on ghost cells via point-to-point communication. This operation has to be performed twice since there may have been \glspl{dof} on ghost cells of which the owning process did not know the correct indices of after the first exchange.

%The enumeration of dofs faces fundamental problems in both parallelisation and \hp-adaptive methods individually. Let us first elaborate on those and quickly recall the corresponding papers, before moving to the actual consolidation of the two.

For parallel \hp-adaptive \gls{fem}, the mere concatenation of both algorithms does not suffice. The case in which distinct finite element types from different subdomains are adjacent has to be given special consideration. We could cope with this situation by constraining \glspl{dof} on these interfaces to each other. However, this would leave unnecessary \gls{dof} duplicates in the equation system. Additionally, the global number of \glspl{dof} would differ with the number of subdomains in this approach. We would rather keep it independent from the number of processes, which would simplify debugging and assures that our solvers yield the same results on any number of subdomains. The algorithm developed in this thesis meets this requirement and combines the ideas of both previous algorithms.

A suitable benchmark that we used to test the enumeration algorithm consists of a two-dimensional mesh of four neighboring cells. We provide two different Lagrangian elements that share at least one additional \gls{dof} per cell interface than only on vertices. For this purpose, we choose Lagrangian elements $Q_2$ and $Q_4$. Each of these finite elements will be assigned to two catty-cornered cells. Furthermore, we divide the mesh into two subdomains containing two neighboring cells with different reference finite elements. The whole setup is shown in Fig.~\ref{fig:enumbenchmark} and covers all combinations of adjacent finite elements that we have encountered so far in the parallel \hp-adaptive context. A step-by-step demonstration of the algorithm on this particular benchmark is presented in App.~\ref{app::enumeration}.

\begin{figure}
\centering
\input{figures/parallel/enumbenchmark}
\input{figures/parallel/enumbenchmarklegend}
\caption[Benchmark scenario for \glsfmtlong{dof} enumeration.]{Benchmark scenario to verify our algorithm for \gls{dof} enumeration. The mesh is distributed on two \gls{mpi} processes, each owning one $Q_2$ and one $Q_4$ element. All \glspl{dof} are uniquely identified on the global mesh as a result of the enumeration algorithm from Sec.~\ref{sec:enumeration}.}
\label{fig:enumbenchmark}
\end{figure}

The algorithm enumerates all \glspl{dof} on locally relevant cells, which includes ghost cells. Thus, we begin by exchanging active finite element indices on ghost cells so that we have information about all locally relevant finite elements and can prepare all data structures accordingly. We do this with point-to-point communication.

In short, our algorithm is based on the parallel algorithm by \textcite[Sec.~3.1]{bangerth2012} for the most part. We will add a \gls{dof} unification step after enumerating all locally owned cells, while subjecting to the finite element domination hierarchy to decide ownership on all interfaces. After exchanging \glspl{dof} on all ghost cells, we are left to merge the valid \glspl{dof} on interfaces with the valid counterparts.

In detail, the complete algorithm consists of the following six steps, starting with an initialization step flagged with `0`. The step-by-step demonstration presented in App.~\ref{app::enumeration} should be consulted as an aid to its understanding. To follow the same nomenclature as \textcite{bangerth2012}, we call the set of all locally owned cells $\cellsp{loc}$, the set of all ghost cells $\cellsp{ghost}$, and the set of all locally relevant cells $\cellsp{rel} = \cellsp{loc} \cup \cellsp{ghost}$ on processes and subdomains identified by the integer $p$.
\begin{enumerate}
  \item[0.] \textit{Initialization.}
  On all locally relevant cells $K \in \cellsp{rel}$, \gls{dof} indices are set to an invalid value, for example $i \coloneqq -1$.
  \item \textit{Local enumeration.}
  Iterate over all locally owned cells $K \in \cellsp{loc}$ and assign valid \gls{dof} indices in ascending order, starting from zero.
  \item \textit{Tie-break.}
  Go over all locally owned cells $K \in \cellsp{loc}$. If a mesh object on $K$ is also part of an adjacent ghost cell which has the same reference finite element assigned and belongs to a subdomain of lower rank $q < p$, then invalidate all \glspl{dof} on the mesh object by setting their index to the invalid value $i$.
  \item \textit{Unification.}
  Go over all locally owned cells $K \in \cellsp{loc}$. For all shared \glspl{dof} on interfaces to neighboring cells, constrain the \gls{dof} of the dominated finite element to the one of the dominating element. On interfaces to ghost cells, set \glspl{dof} indices to the invalid value $i$ if the dominating element is assigned to the ghost cell. It is possible that none of the adjacent elements dominates, e.g.\ if a $(Q_1 \times Q_2)$ element neighbors a $(Q_2 \times Q_1)$ element as described in Sec.~\ref{sec:prerequisites}. In this case, designate the first active finite element on each mesh object as its dominant one.
  %on interfaces between locally owned cells. On interfaces to ghost cells however, apply the \textit{tie-break} criterion from the previous step: If the ghost cells belongs to a subdomain of lower rank $q < p$, set all \gls{dof} indices on their shared interface to the invalid value $i$.
%  Note that at this stage, we will never overwrite a valid index on ghost interfaces.
  Populate a list with pairs of identifiers for these constrained \glspl{dof}.
%  This has to happen after the tie-break.
\end{enumerate}
At this stage, each process knows which \glspl{dof} are owned by them.
\begin{enumerate}[resume]
  \item \textit{Global re-enumeration.}
  Iterate over all locally owned cells $K \in \cellsp{loc}$
  and enumerate those \gls{dof} indices in ascending order that have a valid value assigned, while considering all constraints from the previous step. Store the number of all valid \gls{dof} indices on this subdomain as $n_p$. In a next step, shift all indices by the number of \glspl{dof} that are owned by all processors of lower rank $q < p$, or in other words, by $\sum_{q=0}^{p-1} n_q$. This corresponds to a prefix sum or exclusive scan, and can be obtained via \texttt{MPI\_Exscan} \textcite{mpi31}. Note that simply shifting indices as in the algorithm without \p-adaptivity is no longer sufficient.
\end{enumerate}
At this stage, all subdomains and processes have the correct indices assigned to all locally owned \glspl{dof}, which are enumerated consecutively.
\begin{enumerate}[resume]
  \item \textit{Ghost exchange.}
  Communicate \gls{dof} indices from locally owned cells $K \in \cellsp{loc}$ to other processes using point-to-point communication as follows.
  \begin{enumerate}[label=\alph*.]
%    \item Mark all vertices on locally owned cells $\cellsp{loc}$.
%    \item Go over all vertices on ghost cells $\cellsp{ghost}$ and store all marked vertices and their all processor identifiers.
%    \item Mark all locally owned cells that have ghost neighbors.
    \item Prepare containers with data to be sent from subdomain $p$ to each adjacent subdomain $q$.
    \item Loop over all locally owned cells that have ghost neighbors. Add the cell identifier with all associated \gls{dof} indices to every data container that corresponds to an adjacent subdomain of the current cell.
    \item Send each data container to its designated destination process with nonblocking point-to-point communication via \texttt{MPI\_Isend} \textcite{mpi31}.
    \item Receive data containers from processes of adjacent subdomains with nonblocking point-to-point communication via \texttt{MPI\_Irecv} \textcite{mpi31}. The received data corresponds to all ghost cells of this subdomain $p$. On each of these cells, set the received \gls{dof} indices accordingly.
  \end{enumerate}
  All communication in this step is symmetric, which means that a process only receives data from another process when it also sends data to it. Thus, there is no need to negotiate communication.
\end{enumerate}
After the previous ghost exchange each \gls{dof} on interfaces with ghost cells has exactly one valid index assigned.
\begin{enumerate}[resume]
  \item \textit{Merge on interfaces.}
  Go over all locally relevant cells $K \in \cellsp{rel}$. On interfaces between locally owned and ghost cells, set all remaining invalid \gls{dof} indices with the corresponding valid one of the dominating finite element.
\end{enumerate}
At this stage, all locally owned cells $K \in \cellsp{loc}$ have their correct \gls{dof} indices set.
\begin{enumerate}[resume]
  \item \textit{Ghost exchange.}
  Some ghost cells may still have invalid \gls{dof} indices assigned since the owning process did not know all correct indices on this particular cell when the last communication happened. Another ghost exchange closes this gap by repeating step 5. However this time, only data from those cells has to be communicated which had invalid \gls{dof} indices prior to step 5d.
\end{enumerate}
One variant of this algorithm would be to modify steps 2 and 3 inasmuch as we apply the \textit{tie-break} criterion on all \glspl{dof} on ghost interfaces and exclude them from \gls{dof} unification entirely. However, this would not assign shared \glspl{dof} to the dominating finite element on ghost interfaces, which would be inconsistent compared to the locally owned parts of the domain.

At the end of this algorithm, all global \gls{dof} indices have been set correctly, and every process knows the indices of all locally relevant \glspl{dof}: All \glspl{dof} on interfaces belong to the dominating finite element on the process with the smallest rank. In particular, while the algorithm is substantially more complicated than the one without \p-adaptivity, no additional communication steps are introduced to the two original ones from the \h-adaptive variant.

There may be situations where \glspl{dof} are constrained to others, which in turn are constrained to different third ones. These chains of constraints may span over \glspl{dof} from multiple subdomains. To deal with this case, we might need more than the current two communication steps in our algorithm. However, we could not think of any scenario in which this is going to be the case, and did not encounter any issues in our investigations so far.

In three-dimensional scenarios, \textcite[Sec.~4.6]{bangerth2009} pointed out possible complications with circular constraints during \gls{dof} unification whenever three or more different finite elements share a common edge. We still have not figured out a satisfactory solution for this problem and conform to their original way to cope with this situation: All \glspl{dof} on these edges will be excluded from the unification step and will be treated separately via constraints.